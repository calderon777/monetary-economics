[
  {
    "objectID": "week1_quiz.html",
    "href": "week1_quiz.html",
    "title": "End of Week 1 Quiz",
    "section": "",
    "text": "Note\n\n\n\n\nThis quiz is for self-assessment.\nYour answers are not automatically collected.\nSelect the best answer for each question.\n\n\n\n\n\n\n\n1) According to some accounts, an early form of accounting that recorded national wealth was represented by:\nr webexercises::longmcq(c(\"Cattle\", \"Paper currency\", answer = \"Tally sticks\", \"Jewellery\")) \n\n\nHint\n\nCarved marks as receipts.\n\n\n\n2) In an economy with 6 goods, the number of possible exchange ratios would be:\nr webexercises::longmcq(c(\"6\", \"12\", answer = \"15\", \"30\")) \n\n\nHint\n\nIt‚Äôs n(n-1)/2.\n\n\n\n3) A currency based on fiat money differs from one based on commodity money by the fact that:\nr webexercises::longmcq(c(   \"The former uses paper while the latter uses metallic coins\",   answer = \"The bearer of commodity money has a claim to an equivalent value of the commodity itself, while the bearer of fiat money has no such claim\",   \"The former is issued by a central bank while the latter is not\",   \"The former circulates as a medium of exchange only because it is universally accepted as playing this role while the latter does not require such acceptance\" )) \n\n\nHint\n\nOne is redeemable, the other is not.\n\n\n\n4) Which of the following properties is an objective criterion for something to serve as a medium of exchange?\nr webexercises::longmcq(c(\"Marketability\", \"Backing by a Central Bank\", \"Predictability of exchange value\", answer = \"Durability\")) \n\n\nHint\n\nThink physical robustness.\n\n\n\n5) Which of the following statements reflects the debate between Keynesian and Classical economists?\nr webexercises::longmcq(c(   answer = \"The use of money as a store of value can lead to profound implications for macroeconomic stability\",   \"Every trader is willing to accept money as a means of payment precisely because every other trader is willing to accept money as a means of payment\",   \"The use of a common unit of account for all items traded in an economy makes it easier to keep track of the exchange ratios between these items\",   \"The use of money as a medium of exchange automatically makes it a store of value\" )) \n\n\nHint\n\nLiquidity preference vs neutrality."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Monetary Economics",
    "section": "",
    "text": "Welcome to Monetary Economics.\nThis site will contain:"
  },
  {
    "objectID": "index.html#lecture-notes",
    "href": "index.html#lecture-notes",
    "title": "Monetary Economics",
    "section": "Lecture notes",
    "text": "Lecture notes"
  },
  {
    "objectID": "index.html#quizzes",
    "href": "index.html#quizzes",
    "title": "Monetary Economics",
    "section": "Quizzes",
    "text": "Quizzes\n\nEnd of Week 1 Quiz"
  },
  {
    "objectID": "index.html#lab-materials-r-data-work",
    "href": "index.html#lab-materials-r-data-work",
    "title": "Monetary Economics",
    "section": "Lab materials (R / data work)",
    "text": "Lab materials (R / data work)\n\nSup test"
  },
  {
    "objectID": "index.html#extra-readings-and-links",
    "href": "index.html#extra-readings-and-links",
    "title": "Monetary Economics",
    "section": "Extra readings and links",
    "text": "Extra readings and links"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "supervision_2.html",
    "href": "supervision_2.html",
    "title": "Supervision 2",
    "section": "",
    "text": "1) What relationship does simple linear regression assume between y and x?\n\n\n Quadratic Logarithmic Linear Exponential\n\n\n\n\nHint\n\nThink straight line: one slope + one intercept.\n\nŒ≤\n\n\n2) In the model y = Œ≤‚ÇÄ + Œ≤‚ÇÅ x + Œµ, what does Œ≤‚ÇÅ represent?\n\n\n The average of y The error variance Expected change in y for a one-unit increase in x The intercept\n\n\n\n\nHint\n\nŒîy for +1 in x.\n\n\n\n3) In the model y = Œ≤‚ÇÄ + Œ≤‚ÇÅ x + Œµ, what does Œ≤‚ÇÄ represent?\n\n\n The slope of x The expected value of y when x = 0 The variance of Œµ Correlation between x and y\n\n\n\n\nHint\n\nValue of y at x = 0.\n\n\n\n4) A residual is defined as:\n\n\n Predicted y minus observed y Observed x minus predicted x Observed y minus predicted y The fitted value\n\n\n\n\nHint\n\nActual ‚àí fitted.\n\n\n\n5) Ordinary Least Squares (OLS) chooses coefficients to:\n\n\n Maximize R¬≤ Minimize the sum of absolute residuals Minimize the sum of squared residuals Maximize the likelihood under normal errors only\n\n\n\n\nHint\n\nSquares penalize big errors more.\n\n\n\n6) With an intercept in the model, OLS residuals:\n\n\n Sum to a positive number Sum to a negative number Sum to zero Sum to the sample mean of y\n\n\n\n\nHint\n\n‚àëe·µ¢ = 0 and X·µÄe = 0 when an intercept is included.\n\n\n\n7) In simple OLS with an intercept, the fitted line passes through‚Ä¶\n\n\n The origin (0,0) The point (xÃÑ, »≥) The median of x and y The maximum of y\n\n\n\n\nHint\n\nIt goes through the means.\n\n\n\n8) Which R function fits a simple linear regression model?\n\n\n plot() cor() lm() predict()\n\n\n\n\n9) In summary(mod), the p-value for the slope tests:\n\n\n H‚ÇÄ: Œ≤‚ÇÅ ‚â† 0 H‚ÇÄ: Œ≤‚ÇÅ = 0 H‚ÇÄ: Œ≤‚ÇÄ = 0 H‚ÇÄ: errors are normal\n\n\n\n\n10) The p-value is best described as:\n\n\n The probability H‚ÇÅ is true The probability the estimate is correct The probability (under H‚ÇÄ) of a result at least as extreme as observed The Type II error rate\n\n\n\n\n11) A 95% confidence interval for Œ≤‚ÇÅ is generally:\n\n\n Estimate ¬± 1.96 √ó œÉ of y Estimate ¬± critical t √ó SE(Œ≤‚ÇÅ) Estimate ¬± 2 √ó residual SD SE(Œ≤‚ÇÅ) ¬± estimate\n\n\n\n\n12) R¬≤ measures:\n\n\n The correlation between x and y Proportion of variance in y explained by x The average residual size The probability the model is correct\n\n\n\n\n13) Adjusted R¬≤ differs from R¬≤ because it:\n\n\n Ignores sample size Is always larger than R¬≤ Penalizes adding predictors relative to sample size Equals 1 ‚àí R¬≤\n\n\n\n\n14) To make a prediction at x = x‚ÇÄ, you use:\n\n\n Œ≤‚ÇÅ √ó x‚ÇÄ Œ≤‚ÇÄ √∑ x‚ÇÄ ≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó x‚ÇÄ SE(Œ≤‚ÇÅ) √ó x‚ÇÄ\n\n\n\n\nHint\n\nPlug x‚ÇÄ into the fitted line.\n\n\n\n15) For OLS to be unbiased, a key assumption is:\n\n\n x is normally distributed Errors have zero variance E[Œµ | x] = 0 (errors have zero mean conditional on x) y is standardized\n\n\n\n\n16) Homoskedasticity means:\n\n\n Errors are perfectly correlated Variance of errors is constant across x Mean of errors is zero only at xÃÑ x has constant variance\n\n\n\n\n17) In summary(mod), the t value for Œ≤‚ÇÅ equals:\n\n\n Œ≤‚ÇÅ SE(Œ≤‚ÇÅ) Estimate(Œ≤‚ÇÅ) / SE(Œ≤‚ÇÅ) 1 ‚àí p-value\n\n\n\n\nHint\n\nSignal √∑ noise for the slope estimate.\n\n\n\n18) The units of Œ≤‚ÇÅ are best described as:\n\n\n Units of y Units of y per unit of x Unitless Units of x per unit of y\n\n\n\n\n19) Rescaling x from dollars to hundreds of dollars will:\n\n\n Leave Œ≤‚ÇÅ unchanged Rescale Œ≤‚ÇÅ numerically but leave R¬≤ and the t-test for Œ≤‚ÇÅ unchanged Change the data but not the model Invalidate OLS\n\n\n\n\n20) After fitting mod &lt;- lm(y ~ x, d), the fitted value at x‚ÇÄ is:\n\n\n residuals(mod)[x==x‚ÇÄ] predict(mod, newdata = data.frame(x = x‚ÇÄ)) coef(mod)['x'] fitted(mod)[1] always\n\n\n\n\nHint\n\nUse predict() with a small data frame for x‚ÇÄ.\n\n\n\n\nüèÅ End of lab 7 üõë Remember to save your script üíæ"
  },
  {
    "objectID": "supervision_2.html#lab-8-prediction-with-the-linear-regression-model",
    "href": "supervision_2.html#lab-8-prediction-with-the-linear-regression-model",
    "title": "Supervision 2",
    "section": "üß™ Lab 8 ‚Äî Prediction with the Linear Regression Model",
    "text": "üß™ Lab 8 ‚Äî Prediction with the Linear Regression Model\n\nüéØ Learning outcomes\nBy the end of this lab you will be able to:\n\nGenerate point predictions and quantify their uncertainty using R‚Äôs prediction framework\nDistinguish between confidence intervals for average outcomes and prediction intervals for individual cases, and select the appropriate interval for applied research contexts\nAssess the reliability of coefficient estimates through their variance-covariance structure\nRecognize when predictions involve extrapolation beyond observed data and understand the associated risks\nHandle non-linear relationships through quadratic and log-linear specifications\n\n\n\n\nüß∞ Prerequisites\nKnowledge: - Understanding of simple linear regression (Lab 7) - Familiarity with residuals, fitted values, and R¬≤ - Basic probability concepts (sampling distributions, standard errors)\nTechnical: - R (‚â• 4.0) and RStudio installed - Completed Labs 1-7 - Required packages (install if needed):\ninstall.packages(c(\"remotes\", \"PoEdata\")) \nremotes::install_github(\"ccolonescu/PoEdata\")\nlibrary(PoEdata)\nDatasets:\nfood - household food expenditure and income (40 observations) br - Baton Rouge house prices and characteristics (1,080 observations) ‚Äî\n\n\n\n\n\n\nNote\n\n\n\nNotation reminder: We use Greek letters (Œ≤1,Œ≤2‚Äã) for true population parameters and Latin letters (b1,b2‚Äã) for their sample estimates.\n≈∑: predicted y value\n\\(\\hat{\\beta}\\) or \\(\\hat{b}\\) are sometimes used interchangeably with b for estimates\n\n\nIn Lab 7, we learned how to estimate relationships, But estimation is only half the story. In applied research, we often need to make predictions. In applied land economics research, prediction is crucial‚Äîwhether forecasting property values, estimating rental yields, or projecting land use changes. This lab equips you with the tools to make robust predictions and quantify their uncertainty. Let‚Äôs begin with the basic prediction workflow.\nOnce we have estimated our regression coefficients \\(b_1\\) (intercept) and \\(b_2\\) (slope), we can use them to predict food expenditure for any given income level using the fitted regression equation (Eq. 2).\n\\[\n\\hat{y} = b_1 + b_2 x \\tag{2}\n\\]\n‚ö†Ô∏èUnit conversion: In the following R script ‚Äúincome = $2000‚Äù is ‚Äúincome = 20‚Äù (the data is in hundreds of dollars).\n\n#### Step 1: Fit the model (recap from Lab 7)\n\nlibrary(PoEdata)\ndata(\"food\")\n\n# Estimate the food expenditure model\nmod1 &lt;- lm(food_exp ~ income, data = food)\n\n# Review estimates\nsummary(mod1)\n\n\nCall:\nlm(formula = food_exp ~ income, data = food)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-223.025  -50.816   -6.324   67.879  212.044 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   83.416     43.410   1.922   0.0622 .  \nincome        10.210      2.093   4.877 1.95e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 89.52 on 38 degrees of freedom\nMultiple R-squared:  0.385, Adjusted R-squared:  0.3688 \nF-statistic: 23.79 on 1 and 38 DF,  p-value: 1.946e-05\n\n#### Step 2: Create a data frame with target income values\n\n# Scenario: Predict food expenditure for three household types\n# Note: income is in $100s, so divide actual income by 100\nnewx &lt;- data.frame(income = c(20, 25, 27))\n\n#### Step 3: Generate predictions\n\n# The predict() function takes two arguments:\n#   1. A fitted model object (mod1)\n#   2. A data frame with new x-values (must have same column names as original data)\nyhat &lt;- predict(mod1, newx)\n\n# Give friendly names to each prediction so the output is   easy to read\nnames(yhat) &lt;- c(\"Low income = $2000\", \"Median income = $2500\", \"High income = $2700\")\n\n# Show the predictions\nyhat\n\n   Low income = $2000 Median income = $2500   High income = $2700 \n             287.6089              338.6571              359.0764 \n\n#### Step 4: Interpret the results\n\n# A household earning $2,000/week is predicted to spend $287.60 on food per week.\n# This represents about 14.4% of their income (287.6/2000).\n\n# For the median household ($2,500/week), predicted food expenditure is $338.70,\n# or about 13.5% of income.\n\n# Observation: The proportion of income spent on food decreases as income rises‚Äî\n# this is consistent with Engel's Law from economics.\n\nWhat‚Äôs Missing? Uncertainty!\nThese are point predictions‚Äîour single best guess at each income level. But how confident are we?\nConsider the household earning $2,000/week: - We predict they‚Äôll spend $287.60 on food - But there‚Äôs sampling uncertainty (our \\(b_1\\) and \\(b_2\\) are estimates) - And individual variation (not all $2,000/week households spend exactly $287.60)\nTwo types of intervals address these concerns:\n\nConfidence intervals answer: ‚ÄúWhat‚Äôs the average food expenditure for all households at this income level?‚Äù\nPrediction intervals answer: ‚ÄúWhat might one specific household at this income level spend?‚Äù\n\n\n\n\nUnderstanding Prediction Uncertainty Through Sampling Variability\nOur predictions depend entirely on our estimated coefficients (\\(b_1, b_2\\)). But these are sample estimates‚Äîif we collected a different dataset, we‚Äôd get different values and therefore different predictions.\nBefore making predictions, it‚Äôs important to understand that our coefficient estimates vary across samples. This matters for land economics applications: if we‚Äôre advising on property valuations, we need to know how sensitive our predictions are to sampling variation.\n\n\n\n\n\n\nWhat is ‚Äúsampling with replacement‚Äù?\n\n\n\nImagine our 40 observations are numbered balls in an urn. We draw a ball, record its data, put it back, then draw again. Some observations may appear multiple times in a bootstrap sample; others not at all. This mimics the randomness of drawing a new sample from the population.\n\n\nThe following bootstrap exercise (simulating sampling variability) demonstrates this variability by repeatedly resampling our data and re-estimating the model:\n\nN &lt;- nrow(food)   # observations\nC &lt;- 50           # repeats\nS &lt;- 38           # subsample size\n\nsumb2 &lt;- 0        # sum of slopes\nfor (i in 1:C){\n  set.seed(3*i)   # reproducible\n  # Draw a bootstrap sample (with replacement)\n  subsample &lt;- food[sample(1:N, S, replace = TRUE), ]  # bootstrap draw\n  # Fit the model on this bootstrap sample\n  mod2 &lt;- lm(food_exp ~ income, data = subsample)      \n  sumb2 &lt;- sumb2 + coef(mod2)[2]                       # store slope Œ≤2\n}\n\nprint(sumb2 / C, digits = 3)   # average slope\n\nincome \n  9.89 \n\n\nCompare this bootsrap average (repeated samples) witht the original OLS regression output (from mod1) for \\(b_2\\). They should be similar, confirming that OLS is unbiased.\n\n\nEstimated Variances and Covariance of Coefficients\nThe variance-covariance matrix tells us two crucial things: 1. Variances (diagonal): How much each coefficient estimate varies 2. Covariances (off-diagonal): How the estimates move together\nThis matters for prediction because uncertainty in \\(\\hat{y}\\) depends on uncertainty in both coefficients and their correlation. For land valuation models, ignoring this covariance can lead to overconfident predictions.\n\n\n\n\n\n\nWarning\n\n\n\nCommon Error in Applied Work Many analysts incorrectly calculate prediction variance as just \\(\\text{Var}(b_1) + x_0^2 \\cdot \\text{Var}(b_2)\\), omitting the covariance term. This can make predictions appear less precise than they actually are.\n\n\nThe following R script extracts estimated variances and covariances from the object mod 1.\n\n# coef variance‚Äìcovariance matrix.\n# This is a 2√ó2 symmetric matrix:\n# - Top-left (1884.44): Var(b‚ÇÅ), variance of intercept\n# - Bottom-right (4.38): Var(b‚ÇÇ), variance of slope  \n# - Off-diagonals (-87.78): Cov(b‚ÇÅ,b‚ÇÇ), covariance between coefficients\nvcov(mod1)                       \n\n            (Intercept)     income\n(Intercept)  1884.44226 -85.903157\nincome        -85.90316   4.381752\n\n#### Extract individual components\n(varb1   &lt;- vcov(mod1)[1, 1])        # Var(B1), intercept variance\n\n[1] 1884.442\n\n(varb2   &lt;- vcov(mod1)[2, 2])        # Var(B2), slope variance\n\n[1] 4.381752\n\n(covb1b2 &lt;- vcov(mod1)[1, 2])        # Cov(B1, B2), covariance\n\n[1] -85.90316\n\n\nHaving established how to assess coefficient stability, we now turn to cases where linear relationships are insufficient to capture real-world patterns in land markets.\n\n\nNon-Linear Relationships - When Straight Lines Don‚Äôt Fit\nIn practice, many land economic relationships are non-linear: - Property prices don‚Äôt increase linearly with size (diminishing returns to scale) - Land values may have threshold effects near transport nodes - Agricultural productivity often follows diminishing returns\nWe‚Äôll explore two common approaches Quadratic Models and Log-Linear Models.\n\nPART A. Quadratic model - for U-shaped or inverted-U relationships\n\\[\ny_i = \\beta_1 + \\beta_2 x_i^2 + e_i.\\tag{5}\n\\]\n\nlibrary(PoEdata)            # load package\ndata(br)                    # dataset\n\nmod3 &lt;- lm(price ~ I(sqft^2), data = br)  # fit model\nb1 &lt;- coef(mod3)[1]        # intercept\nb2 &lt;- coef(mod3)[2]        # coeff on sqft^2\n\nsqftx &lt;- c(2000, 4000, 6000)              # evaluation points\npricex &lt;- b1 + b2 * sqftx^2               # predicted price\nDpriceDsqft &lt;- 2 * b2 * sqftx             # marginal effect d(price)/d(sqft)\nelasticity &lt;- (DpriceDsqft * sqftx) / pricex  # elasticity\n\nb1; b2; DpriceDsqft; elasticity           # output\n\n(Intercept) \n   55776.57 \n\n\nI(sqft^2) \n0.0154213 \n\n\n[1]  61.68521 123.37041 185.05562\n\n\n[1] 1.050303 1.631251 1.817408\n\n\nPlotting two alternatives for the quadratic fit:\n\nmod31 &lt;- lm(price ~ I(sqft^2), data = br)                # fit\nplot(br$sqft, br$price, col = \"grey\",\n     xlab = \"Total square feet\", ylab = \"Sale price, $\") # scatter\n\nb &lt;- coef(mod31)                                         # [intercept], sqft^2\ncurve(b[1] + b[2]*x^2,\n      from = min(br$sqft), to = max(br$sqft),            # range\n      add = TRUE, lwd = 2)                               # fitted curve\n\n\n\n\n\n\n\n\n\nordat &lt;- br[order(br$sqft), ]                             # sort by sqft\nmod31 &lt;- lm(price ~ I(sqft^2), data = ordat)              # fit model on sorted data\n\nplot(br$sqft, br$price, col = \"grey\",                     # scatter\n     main = \"Dataset ordered by 'sqft'\",\n     xlab = \"Total square feet\", ylab = \"Sale price, $\")\n\nlines(fitted(mod31) ~ ordat$sqft)                         # add fitted curve\n\n\n\n\n\n\n\n\n\n\nPART B. Log-Linear Models - for proportional (percentage) relationships\n\\[\n\\log(y_i) = \\beta_1 + \\beta_2 x_i + e_i.\\tag{6}\n\\]\n\nhist(br$price)                # price distribution\n\n\n\n\n\n\n\nhist(log(br$price))           # log-price distribution\n\n\n\n\n\n\n\nmod4 &lt;- lm(log(price) ~ sqft, data = br)  # log-linear fit\nb1 &lt;- coef(mod4)[1]          # intercept\nb2 &lt;- coef(mod4)[2]          # slope\n\n# Back-transform fitted curve to price scale\nordat &lt;- br[order(br$sqft), ]                     # sort by sqft\nmod4  &lt;- lm(log(price) ~ sqft, data = ordat)      # refit (ordered)\nplot(br$sqft, br$price, col = \"grey\",\n     xlab = \"Total square feet\", ylab = \"Sale price, $\")  # scatter\nlines(exp(fitted(mod4)) ~ ordat$sqft, lwd = 2)    # exp of fitted log-price\n\n\n\n\n\n\n\n# ‚ö†Ô∏è Important: Simply exponentiating log predictions (exp(fitted)) gives the median, not the mean. For mean predictions, apply a smearing correction\n\nElasticity and marginal effect at the median price:\n\npricex &lt;- median(br$price)                                   # target price (median)\nsqftx  &lt;- (log(pricex) - coef(mod4)[1]) / coef(mod4)[2]      # back out sqft from log model\n(DyDx &lt;- pricex * coef(mod4)[2])                              # marginal effect d(price)/d(sqft) = b2 * price\n\n    sqft \n53.46495 \n\n(elasticity &lt;- sqftx * coef(mod4)[2])                         # elasticity = (dP/dx)*(x/P) = b2 * sqft\n\n(Intercept) \n  0.9366934 \n\n\nMultiple points:\n\nb1 &lt;- coef(mod4)[1]                     # intercept of log(price) ~ sqft\nb2 &lt;- coef(mod4)[2]                     # slope wrt sqft\n\nsqftx  &lt;- c(2000, 3000, 4000)           # sqft points\npricex &lt;- c(100000, exp(b1 + b2*sqftx)) # prices: first fixed at 100k; others from model\n\nsqftx  &lt;- (log(pricex) - b1) / b2       # implied sqft from each price (now length = length(pricex))\n(elasticities &lt;- b2 * sqftx)            # elasticity = (dP/dx)*(x/P) = b2 * sqft\n\n[1] 0.6743291 0.8225377 1.2338066 1.6450754\n\n\nüí° Land Economics Application In rental valuation, we rarely observe rental income for vacant plots. Prediction intervals help quantify the uncertainty in forecasted rents, which is crucial for development feasibility analysis.\n‚ö†Ô∏è Common Mistake Students often confuse confidence intervals (for the average) with prediction intervals (for individuals). In property valuation, this distinction matters: are you estimating the average price for houses of this type, or the likely sale price of one specific house?\n\n\nPART C. MCQs\n\n\n1) Using B‚ÇÄ and B‚ÇÅ primarily helps to:\n\n\n Test normality of residuals Estimate œÉ¬≤ Predict E[y|x] Standardize x and y\n\n\n\n\nHint\n\nPlug x into the fitted line.\n\n\n\n2) The function lm() is used to:\n\n\n Simulate data Draw histograms Estimate a linear model Compute correlation only\n\n\n\n\nHint\n\nIt returns coefficients and a model object.\n\n\n\n3) The function predict() mainly:\n\n\n Fits a model Computes residuals only Estimates y-hat for new data Sorts a data frame\n\n\n\n\nHint\n\nYou pass newdata.\n\n\n\n4) Coefficients are random because they:\n\n\n Depend only on fixed formulas Are set by the user Depend on the sample Don‚Äôt change across samples\n\n\n\n\n5) Random subsamples help to:\n\n\n Reduce file size Guarantee higher R¬≤ Evaluate stability/variability Eliminate outliers always\n\n\n\n\n6) vcov(model) returns:\n\n\n Residuals Fitted values Variances and covariances of coefficients Confidence intervals for y-hat\n\n\n\n\n7) Using data.frame() with predict() is to:\n\n\n Provide new x values with correct column names Shuffle the rows Change variable types to character Compute R¬≤\n\n\n\n\n8) To request standard errors from predict.lm you set:\n\n\n stderr = TRUE se = TRUE se.fit = TRUE ci = TRUE\n\n\n\n\nHint\n\nIt returns fit and se.fit.\n\n\n\n9) For a confidence interval for the mean response at x‚ÇÄ, use:\n\n\n interval = 'pi' bands = 'mean' interval = 'confidence' type = 'mean'\n\n\n\n\nHint\n\nCI for E[y|x‚ÇÄ].\n\n\n\n10) For an interval predicting an individual future y at x‚ÇÄ, use:\n\n\n interval = 'confidence' interval = 'prediction' level = 'future' type = 'response'\n\n\n\n\nHint\n\nIncludes error variance.\n\n\n\n11) Prediction intervals are typically:\n\n\n Narrower than confidence intervals Wider than confidence intervals Identical to confidence intervals Unrelated to residual variance\n\n\n\n\n12) confint(model) returns:\n\n\n Intervals for y-hat Confidence intervals for coefficients Prediction intervals for new y Variance of residuals\n\n\n\n\n13) For models with multiple predictors, newdata must:\n\n\n Contain columns matching model terms and types Be a vector in any order Include only the response Be sorted by the response\n\n\n\n\nHint\n\nNames must match the formula‚Äôs variables.\n\n\n\n14) In a log-linear model log(y) ~ x, to back-transform the mean prediction you should:\n\n\n Use only exp(eta) Multiply exp(eta) by a smearing factor Square the fitted values Add residual SD then exponentiate\n\n\n\n\nHint\n\nDuan‚Äôs correction: mean(exp(ŒµÃÇ)).\n\n\n\n15) A bootstrap (or resampling with replacement) in R uses:\n\n\n sample(x, replace = FALSE) sample(x, replace = TRUE) sort(x) order(x)\n\n\n\n\n16) set.seed(123) is used to:\n\n\n Speed up lm() Normalize variables Make random subsamples reproducible Center predictors\n\n\n\n\n17) For out-of-sample performance, a good metric is:\n\n\n Training R¬≤ Test RMSE Number of predictors Intercept size\n\n\n\n\nHint\n\nEvaluate on held-out data.\n\n\n\n18) Extrapolation risk means:\n\n\n Predictions are always unbiased x‚ÇÄ lies outside the observed x range so predictions can be unreliable R¬≤ increases automatically CI and PI become identical\n\n\n\n\n19) Variance of the predicted mean at x‚ÇÄ depends on:\n\n\n Only œÉ¬≤ Only the intercept x‚ÇÄ·µÄ Var(BÃÇ) x‚ÇÄ Only sample size\n\n\n\n\nHint\n\nUse the variance‚Äìcovariance of coefficients.\n\n\n\n20) When predicting with factors, newdata must:\n\n\n Use levels seen in the training data Introduce new unseen levels freely Coerce factors to numeric Omit the factor columns\n\n\n\n\nHint\n\nUnseen levels cause errors in predict().\n\n\n\n\nüèÅ End of lab 8 üõë Remember to save your script üíæ"
  },
  {
    "objectID": "supervision_2.html#lab-9-hypothesis-test-pvalue-testing-linear-combinations",
    "href": "supervision_2.html#lab-9-hypothesis-test-pvalue-testing-linear-combinations",
    "title": "Supervision 2",
    "section": "üß™ Lab 9 ‚Äî Hypothesis Test, p‚ÄëValue & Testing Linear Combinations",
    "text": "üß™ Lab 9 ‚Äî Hypothesis Test, p‚ÄëValue & Testing Linear Combinations\n\nüéØ Learning Outcomes\nBy the end of this lab you will be able to:\n\nFormulate null and alternative hypotheses for regression coefficients\nCompute test statistics and compare them to critical values\nCalculate and interpret p-values for one-tailed and two-tailed tests\nUnderstand the relationship between confidence intervals and hypothesis tests\nTest linear combinations of parameters (e.g., predictions at specific x values)\nApply hypothesis testing to real-world land economics questions\n\n\n\nHypothesis Tests\nüß∞ Prerequisites Knowledge:\nUnderstanding of simple linear regression (Lab 7) Familiarity with coefficient estimates and standard errors (Lab 8) Basic understanding of statistical inference (confidence intervals, significance levels)\nTechnical:\nR (‚â• 4.0) and RStudio installed Completed Labs 1-8 Required packages:\ninstall.packages(c(\"remotes\", \"PoEdata\")) \nremotes::install_github(\"ccolonescu/PoEdata\")\nlibrary(PoEdata)\n\n\n\nIntroduction: Why Hypothesis Testing Matters\nIn Lab 7, we estimated relationships. In Lab 8, we made predictions. But how do we know if our findings are statistically meaningful? Consider our food expenditure model:\nWe estimated that food spending increases by $10.21 for every $100 increase in income But is this effect real, or could it be due to random chance? Could the true effect actually be zero (no relationship)?\nHypothesis testing provides a formal framework to answer these questions.\nüí° Land Economics Application When valuing agricultural land, we might find that proximity to roads increases land value by ¬£500 per meter. But before advising clients, we need to know: Is this effect statistically significant, or could it be sampling noise?\n\nPART A - Hypothesis Tests for Individual Coefficients\nWe start with two competing claims: \\[\nH_0: \\beta_k = c, \\qquad H_A: \\beta_k \\ne c.\n\\]\nTest statistic:\nUnder the null hypothesis, the test statistic follows a t-distribution: \\[\nt = \\frac{b_k - c}{\\operatorname{se}(b_k)},\\quad t \\sim t_{N-2}.\\tag{6}\n\\]\nwhere:\n\\(b_k\\)‚Äã is our estimated coefficient\n\\(c\\) is the hypothesized value (zero in our case)\n\\(\\operatorname{se}(b_k)\\) is the standard error of \\(b_k\\)\n\\(N‚àíK\\) is degrees of freedom (N observations, K parameters)\nIntuition: This ratio measures how many standard errors our estimate is away from the hypothesized value.\nExample 1: Two-Tailed Test (Is there any effect?) Question: Does income affect food expenditure? (i.e., is \\(Œ≤2‚â†0\\)?)\n\n# Set significance level\nalpha &lt;- 0.05\n\n# Load data and estimate model\nlibrary(PoEdata); library(xtable); library(knitr)\n\nWarning: package 'xtable' was built under R version 4.4.3\n\n\nWarning: package 'knitr' was built under R version 4.4.3\n\ndata(\"food\")\nmod1 &lt;- lm(food_exp ~ income, data=food)\nsmod1 &lt;- summary(mod1)\n\n# Regression output\ntable &lt;- data.frame(xtable(mod1))\nkable(table, caption=\"Regression output showing the coefficients\")\n\n\nRegression output showing the coefficients\n\n\n\nEstimate\nStd..Error\nt.value\nPr‚Ä¶t..\n\n\n\n\n(Intercept)\n83.41600\n43.410163\n1.921578\n0.0621824\n\n\nincome\n10.20964\n2.093263\n4.877381\n0.0000195\n\n\n\n\n# Extract coefficient and standard error\nb2 &lt;- coef(mod1)[[\"income\"]]\nseb2 &lt;- sqrt(vcov(mod1)[2,2])\ndf  &lt;- df.residual(mod1)\n\n# Compute test statistic\nt   &lt;- b2/seb2\n\n# Find critical value for two-tailed test\ntcr &lt;- qt(1-alpha/2, df)\n\nt; tcr\n\n[1] 4.877381\n\n\n[1] 2.024394\n\n# Display results\ncat(\"Test statistic:\", round(t, 3), \"\\n\")\n\nTest statistic: 4.877 \n\ncat(\"Critical value (¬±):\", round(tcr, 3), \"\\n\")\n\nCritical value (¬±): 2.024 \n\ncat(\"Decision:\", ifelse(abs(t) &gt; tcr, \"Reject H0\", \"Fail to reject H0\"), \"\\n\")\n\nDecision: Reject H0 \n\n\nInterpretation:\n\nOur test statistic is r round(t, 2), which is much larger than the critical value ¬±r round(tcr, 2)\nWe reject the null hypothesis\nThere is strong evidence that income affects food expenditure\nRight‚Äëtail and left‚Äëtail versions:\n\nOptional: üìä Visualizing the Test:\n\n# Create visualization of t-distribution and test\ncurve(dt(x, df), from = -5, to = 5, \n      main = \"Two-Tailed Hypothesis Test\",\n      xlab = \"t-value\", ylab = \"Density\",\n      lwd = 2)\n\n# Shade rejection regions\npolygon(c(-4, seq(-4, -tcr, 0.01), -tcr),\n        c(0, dt(seq(-4, -tcr, 0.01), df), 0),\n        col = \"red\", border = NA, density = 20)\npolygon(c(tcr, seq(tcr, 4, 0.01), 4),\n        c(0, dt(seq(tcr, 4, 0.01), df), 0),\n        col = \"red\", border = NA, density = 20)\n\n# Mark critical values and test statistic\nabline(v = c(-tcr, tcr), col = \"red\", lty = 2, lwd = 2)\n\nabline(v = t, col = \"blue\", lwd = 2)\n\nlegend(\"topright\", \n       legend = c(\"Rejection region (Œ±/2 each)\", \n                  \"Critical values\", \n                  \"Test statistic\"),\n       col = c(\"red\", \"red\", \"blue\"),\n       lty = c(1, 2, 1), lwd = 2, bty = \"n\")\n\n\n\n\n\n\n\n\nExample 2: One-Tailed Tests Sometimes we have a directional hypothesis based on economic theory. Right-Tailed Test: Is the effect of income on food expenditure greater than $5.50?\n\\[\nH_0: \\beta_2 ‚â§ 5.5, \\qquad H_A: \\beta_2 &gt; 5.5.\n\\]\n\n# Hypothesized value\nc &lt;- 5.5\n\n# Compute test statistic\nt_right &lt;- (b2 - c)/seb2\n\n# Critical value (right tail only)\ntcr_right &lt;- qt(1-alpha, df)\n\ncat(\"Test statistic:\", round(t_right, 3), \"\\n\")\n\nTest statistic: 2.25 \n\ncat(\"Critical value:\", round(tcr_right, 3), \"\\n\")\n\nCritical value: 1.686 \n\ncat(\"Decision:\", ifelse(t_right &gt; tcr_right, \"Reject H0\", \"Fail to reject H0\"), \"\\n\")\n\nDecision: Reject H0 \n\n\nLeft-Tailed Test: Is the effect of income on food expenditure less than $15?\n\\[\nH_0: \\beta_2 ‚â• 15, \\qquad H_A: \\beta_2 &lt; 15.\n\\]\n\n# Left-tail: H0: beta2 &gt;= 15; HA: beta2 &lt; 15\nc &lt;- 15\nt_left &lt;- (b2 - c)/seb2\ntcr_left &lt;- qt(alpha, df)\n\nc(t_right=t_right, tcr_right=tcr_right, t_left=t_left, tcr_left=tcr_left)\n\n  t_right tcr_right    t_left  tcr_left \n 2.249904  1.685954 -2.288463 -1.685954 \n\ncat(\"Test statistic:\", round(t_left, 3), \"\\n\")\n\nTest statistic: -2.288 \n\ncat(\"Critical value:\", round(tcr_left, 3), \"\\n\")\n\nCritical value: -1.686 \n\ncat(\"Decision:\", ifelse(t_left &lt; tcr_left, \"Reject H0\", \"Fail to reject H0\"), \"\\n\")\n\nDecision: Reject H0 \n\n\n‚ö†Ô∏è Common Mistake: Students often confuse the direction of the inequality in \\(H_0\\)‚Äã and \\(H_A\\). Remember: the alternative hypothesis represents what you‚Äôre trying to find evidence for.\n\n\n\nPART B - The p‚ÄëValue Approach\nThe p-value represents the probability of observing a t-statistic as extreme as, or more extreme than, the one obtained from the sample, assuming the null hypothesis is true. In other words, it measures how compatible the data are with the null hypothesis. We reject the null hypothesis when the p-value is smaller than the chosen significance level (e.g., 0.05).\nFor a right-tailed test, the p-value corresponds to the area to the right of the calculated t-statistic. For a left-tailed test, it is the area to the left of the calculated t-statistic. For a two-tailed test, the total p-value is divided equally between both tails‚Äîp/2 in the left tail and p/2 in the right tail.\nIn R, p-values are obtained using the function pt(t, df), where t is the calculated t-ratio and df is the model‚Äôs degrees of freedom. For two-tailed tests, the result should be adjusted to account for both tails, typically using 2*(1 - pt(abs(t), df)).\nDecision rule: Reject \\(H_0\\)‚Äã if \\(p-value&lt;Œ±p\\) Fail to reject \\(H_0\\)‚Äã if \\(p-value‚â•Œ±p\\)\n\n\n\n\n\n\nCommon Significance Levels\n\n\n\n\nŒ± = 0.05 (most common): We‚Äôre willing to accept a 5% chance of incorrectly rejecting H‚ÇÄ\nŒ± = 0.01 (more conservative): Only 1% chance of Type I error\nŒ± = 0.10 (more lenient): Accept 10% chance of error\n\nThe smaller the p-value, the stronger the evidence against H‚ÇÄ\n\n\nComputing p-Values: \\(F_t\\) stands for the cumulative distribution function (CDF) of the t-distribution.\nRight‚Äëtail: \\(p = 1 - F_t(t)\\) (probability in the upper tail)\nLeft‚Äëtail: \\(p = F_t(t)\\) (probability in the lower tail)\nTwo‚Äëtail: \\(p = 2 * (1-F_t(|t|)\\) (probability in both tails)\nExample 1: Right-Tail Test Question: Is the marginal effect of income on food expenditure greater than $5.50?\n\\[H_0: \\beta_2 \\leq 5.5 \\quad \\text{vs} \\quad H_A: \\beta_2 &gt; 5.5\\]\n\n# Right-tail test (H0: Œ≤2 ‚â§ 5.5)\nc &lt;- 5.5\nt &lt;- (b2-c)/seb2\np_right &lt;- 1-pt(t, df)\n\ncat(\"=== RIGHT-TAIL TEST ===\\n\")\n\n=== RIGHT-TAIL TEST ===\n\ncat(\"H0: Œ≤2 ‚â§\", c, \"  (effect is at most $5.50)\\n\")\n\nH0: Œ≤2 ‚â§ 5.5   (effect is at most $5.50)\n\ncat(\"HA: Œ≤2 &gt;\", c, \"  (effect exceeds $5.50)\\n\\n\")\n\nHA: Œ≤2 &gt; 5.5   (effect exceeds $5.50)\n\ncat(\"Test statistic:\", round(t, 3), \"\\n\")\n\nTest statistic: 2.25 \n\ncat(\"p-value:\", round(p_right, 4), \"\\n\")\n\np-value: 0.0152 \n\ncat(\"Decision at Œ±=0.05:\", ifelse(p_right &lt; 0.05, \"‚úì REJECT H0\", \"‚úó FAIL TO REJECT H0\"), \"\\n\")\n\nDecision at Œ±=0.05: ‚úì REJECT H0 \n\ncat(\"\\nInterpretation:\", ifelse(p_right &lt; 0.05, \n           \"Strong evidence that Œ≤2 &gt; 5.5 (the effect exceeds $5.50)\", \"Insufficient evidence that Œ≤2 &gt; 5.5\"), \"\\n\\n\")\n\n\nInterpretation: Strong evidence that Œ≤2 &gt; 5.5 (the effect exceeds $5.50) \n\n\nExample 2: Left-Tail Test\nQuestion: Is the marginal effect of income on food expenditure less than $15?\n\\[H_0: \\beta_2 \\geq 15 \\quad \\text{vs} \\quad H_A: \\beta_2 &lt; 15\\]\n\n# Left-tail test (H0: Œ≤2 ‚â• 15)\nc &lt;- 15\nt &lt;- (b2-c)/seb2\np_left &lt;- pt(t, df)\n\ncat(\"=== LEFT-TAIL TEST ===\\n\")\n\n=== LEFT-TAIL TEST ===\n\ncat(\"H0: Œ≤2 ‚â•\", c, \"  (effect is at least $15)\\n\")\n\nH0: Œ≤2 ‚â• 15   (effect is at least $15)\n\ncat(\"HA: Œ≤2 &lt;\", c, \"  (effect is below $15)\\n\\n\")\n\nHA: Œ≤2 &lt; 15   (effect is below $15)\n\ncat(\"Test statistic:\", round(t, 3), \"\\n\")\n\nTest statistic: -2.288 \n\ncat(\"p-value:\", round(p_left, 4), \"\\n\")\n\np-value: 0.0139 \n\ncat(\"Decision at Œ±=0.05:\", ifelse(p_left &lt; 0.05, \"‚úì REJECT H0\", \"‚úó FAIL TO REJECT H0\"), \"\\n\")\n\nDecision at Œ±=0.05: ‚úì REJECT H0 \n\ncat(\"\\nInterpretation:\", \n    ifelse(p_left &lt; 0.05, \n           \"Strong evidence that Œ≤2 &lt; 15 (the effect is below $15)\",\n           \"Insufficient evidence that Œ≤2 &lt; 15\"), \"\\n\\n\")\n\n\nInterpretation: Strong evidence that Œ≤2 &lt; 15 (the effect is below $15) \n\n\nExample 3: Two-Tail Test (Most Common)\nQuestion: Is there any relationship between income and food expenditure?\n\\[H_0: \\beta_2 = 0 \\quad \\text{vs} \\quad H_A: \\beta_2 \\neq 0\\]\n\n# Two-tail test (H0: Œ≤2 = 0)\nc &lt;- 0  \nt &lt;- (b2-c)/seb2\np_two &lt;- 2*(1-pt(abs(t), df))\n\ncat(\"=== TWO-TAIL TEST ===\\n\")\n\n=== TWO-TAIL TEST ===\n\ncat(\"H0: Œ≤2 = 0  (no relationship)\\n\")\n\nH0: Œ≤2 = 0  (no relationship)\n\ncat(\"HA: Œ≤2 ‚â† 0  (income affects food expenditure)\\n\\n\")\n\nHA: Œ≤2 ‚â† 0  (income affects food expenditure)\n\ncat(\"Test statistic:\", round(t, 3), \"\\n\")\n\nTest statistic: 4.877 \n\ncat(\"p-value:\", format.pval(p_two, digits = 4), \"\\n\")\n\np-value: 1.946e-05 \n\ncat(\"Decision at Œ±=0.05:\", ifelse(p_two &lt; 0.05, \"‚úì REJECT H0\", \"‚úó FAIL TO REJECT H0\"), \"\\n\")\n\nDecision at Œ±=0.05: ‚úì REJECT H0 \n\ncat(\"\\nInterpretation:\", \n    ifelse(p_two &lt; 0.05, \n           \"Strong evidence of a relationship between income and food expenditure\",\n           \"Insufficient evidence of a relationship\"), \"\\n\\n\")\n\n\nInterpretation: Strong evidence of a relationship between income and food expenditure \n\n\n\n\n\n\n\n\n\nSummary of All Three Tests\n\n\n\nSUMMARY TABLE\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nTest Type        | H0          | p-value | Decision\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nRight-tail (&gt;)   | Œ≤2 ‚â§ 5.5    | %.4f    | if p_right &lt; 0.05, \"REJECT\"\nLeft-tail (&lt;)    | Œ≤2 ‚â• 15     | %.4f    | if p_left &lt; 0.05, \"REJECT\"\nTwo-tail (‚â†)     | Œ≤2 = 0      | %.4f    | if p_two &lt; 0.05, \"REJECT\"\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n\nüí° Interpretation Guide:\n\\(p &lt; 0.01\\): Very strong evidence against \\(H_0\\)\n‚Äã \\(0.01 ‚â§ p &lt; 0.05\\): Strong evidence against \\(H_0\\)\n‚Äã \\(0.05 ‚â§ p &lt; 0.10\\): Weak evidence against \\(H_0\\)\n‚Äã \\(p ‚â• 0.10\\): Insufficient evidence against \\(H_0\\)\n‚ö†Ô∏è Common Student Mistakes to Avoid:\n\n‚Äúp-value is the probability that H‚ÇÄ is true‚Äù ‚Üí ‚ùå WRONG!\n\nCorrect: p-value is the probability of getting our data if H‚ÇÄ were true\n\n‚ÄúFail to reject H‚ÇÄ means H‚ÇÄ is true‚Äù ‚Üí ‚ùå WRONG!\n\nCorrect: We simply don‚Äôt have enough evidence to reject it\n\n‚ÄúReject H‚ÇÄ means HA is proven true‚Äù ‚Üí ‚ùå WRONG!\n\nCorrect: We have strong evidence against H‚ÇÄ, supporting HA\n\nConfusing the direction in one-tailed tests ‚Üí Common error!\n\nRight-tail: Testing if parameter is greater than c\nLeft-tail: Testing if parameter is less than c\n\n\n\n\nPART C - Testing Linear Combinations of Parameters\nWe want to test hypotheses about combinations of parameters, not individual coefficients.Example: What is the expected food expenditure for a household earning $2,000/week (income = 20 in our units)?\n\\[\nL = E(\\text{food\\_exp}\\mid \\text{income}=20) = \\beta_1 + 20\\,\\beta_2\n\\]\nVariance identities:\n\\[\n\\operatorname{var}(aX+bY) = a^2\\,\\operatorname{var}(X) + b^2\\,\\operatorname{var}(Y) + 2ab\\,\\operatorname{cov}(X,Y)\n\\]\n\\[\n\\operatorname{var}(b_1+20b_2) = \\operatorname{var}(b_1) + 20^2\\operatorname{var}(b_2) + 2\\cdot20\\operatorname{cov}(b_1,b_2)\n\\]\n‚ö†Ô∏è Critical Point: Many students forget the covariance term! Omitting it leads to incorrect standard errors.\n\n# Significance level and x-value of interest\nalpha &lt;- 0.05\nx &lt;- 20\n\n# the model\nm1 &lt;- lm(food_exp ~ income, data=food)\n\n\n# Extract coefficients and variance-covariance matrix\nb1 &lt;- m1$coef[1]\nb2 &lt;- m1$coef[2]\nvarb1 &lt;- vcov(m1)[1,1]\nvarb2 &lt;- vcov(m1)[2,2]\ncovb1b2 &lt;- vcov(m1)[1,2]\n\n# Compute the linear combination and its variance\nL &lt;- b1 + b2*x\nvarL &lt;- varb1 + x^2 * varb2 + 2*x*covb1b2\nseL &lt;- sqrt(varL)\n\n# Confidence interval for L\ndf &lt;- df.residual(m1)\ntcr &lt;- qt(1-alpha/2, df)\nlowbL &lt;- L - tcr*seL; upbL &lt;- L + tcr*seL\nc(L=L, seL=seL, low=lowbL, up=upbL)\n\n  L.(Intercept)             seL low.(Intercept)  up.(Intercept) \n      287.60886        14.17804       258.90692       316.31081 \n\ncat(\"Expected food expenditure at income = $2000:\\n\")\n\nExpected food expenditure at income = $2000:\n\ncat(\"  Point estimate:\", round(L, 2), \"\\n\")\n\n  Point estimate: 287.61 \n\ncat(\"  Standard error:\", round(seL, 2), \"\\n\")\n\n  Standard error: 14.18 \n\ncat(\"  95% CI: [\", round(lowbL, 2), \",\", round(upbL, 2), \"]\\n\\n\")\n\n  95% CI: [ 258.91 , 316.31 ]\n\n\n\n\nHypothesis Test for Linear Combination\nQuestion: Is expected food expenditure significantly different from $250 at this income level?\n\\[\nH_0: L = 250, \\qquad H_A: L \\ne 250.\n\\]\n\n# Hypothesized value\nc &lt;- 250\n\n# Test statistic\nt &lt;- (L - c)/seL\n\n# p-value (two-tailed)\np_value &lt;- 2*(1-pt(abs(t), df))\nc(t=t, p_value=p_value)\n\n      t.(Intercept) p_value.(Intercept) \n         2.65261316          0.01159078 \n\ncat(\"Hypothesis test: H0: L = 250\\n\")\n\nHypothesis test: H0: L = 250\n\ncat(\"  Test statistic:\", round(t, 3), \"\\n\")\n\n  Test statistic: 2.653 \n\ncat(\"  p-value:\", round(p_value, 4), \"\\n\")\n\n  p-value: 0.0116 \n\ncat(\"  Decision at Œ±=0.05:\", ifelse(p_value &lt; 0.05, \"Reject H0\", \"Fail to reject H0\"), \"\\n\")\n\n  Decision at Œ±=0.05: Reject H0 \n\n\nInterpretation: We fail to reject the null hypothesis. There is insufficient evidence that expected food expenditure differs from $250 for households earning $2,000/week.\n\n\nPART D - MCQs\n\n\n1) In hypothesis testing, H‚ÇÄ represents:\n\n\n The research hypothesis What we hope to prove The status quo or claim being tested The alternative explanation\n\n\n\n\nHint\n\nThink of it as the ‚Äúinnocent until proven guilty‚Äù claim.\n\n\n\n2) The test statistic t = (b - c) / se(b) measures:\n\n\n The absolute error The confidence level How many standard errors b is from c The sample size\n\n\n\n\nHint\n\nIt‚Äôs a standardized distance.\n\n\n\n3) For a two-tailed test at Œ± = 0.05 with df = 38, the critical value is approximately:\n\n\n 1.645 1.686 ¬±2.024 ¬±1.96\n\n\n\n\nHint\n\nUse qt(0.975, 38) for the upper tail.\n\n\n\n4) A p-value of 0.03 means:\n\n\n H‚ÇÄ has 3% chance of being true There's a 3% chance the result is wrong If H‚ÇÄ were true, there's a 3% chance of results this extreme The effect size is 3%\n\n\n\n\n5) We reject H‚ÇÄ when:\n\n\n p-value &lt; Œ± p-value &gt; Œ± t-statistic &lt; 0 Confidence interval includes zero\n\n\n\n\n6) For a right-tailed test, the p-value is calculated as:\n\n\n pt(t, df) 2 * pt(abs(t), df) 1 - pt(t, df) pt(-t, df)\n\n\n\n\nHint\n\nWe want the upper tail probability.\n\n\n\n7) A 95% confidence interval for Œ≤‚ÇÇ is [3, 8]. In a two-tailed test at Œ± = 0.05, we would:\n\n\n Reject H‚ÇÄ: Œ≤‚ÇÇ = 5 Fail to reject H‚ÇÄ: Œ≤‚ÇÇ = 5 Reject H‚ÇÄ: Œ≤‚ÇÇ = 0 Need more information\n\n\n\n\nHint\n\n5 is inside the interval.\n\n\n\n8) The same confidence interval [3, 8] tells us we would:\n\n\n Fail to reject H‚ÇÄ: Œ≤‚ÇÇ = 0 Reject H‚ÇÄ: Œ≤‚ÇÇ = 0 Accept H‚ÇÄ: Œ≤‚ÇÇ = 10 Reject H‚ÇÄ: Œ≤‚ÇÇ = 6\n\n\n\n\nHint\n\n0 is outside the interval.\n\n\n\n9) To test L = Œ≤‚ÇÅ + 20Œ≤‚ÇÇ, we need:\n\n\n Only var(Œ≤‚ÇÅ) and var(Œ≤‚ÇÇ) Only the covariance Var(Œ≤‚ÇÅ), var(Œ≤‚ÇÇ), and cov(Œ≤‚ÇÅ, Œ≤‚ÇÇ) Just the point estimate\n\n\n\n\n10) The variance of L = a¬∑X + b¬∑Y includes the term:\n\n\n ab ¬∑ var(X) ab ¬∑ var(Y) 2ab ¬∑ cov(X,Y) (a+b)¬≤ ¬∑ var(X)\n\n\n\n\nHint\n\nDon‚Äôt forget the ‚Äú2‚Äù and the product ab.\n\n\n\n11) vcov(model) returns:\n\n\n Only variances Only covariances A matrix with variances on diagonal and covariances off-diagonal Confidence intervals\n\n\n\n\n12) A Type I error occurs when:\n\n\n We fail to reject a false H‚ÇÄ We reject a true H‚ÇÄ Our sample size is too small The p-value is large\n\n\n\n\nHint\n\nIt‚Äôs controlled by Œ±.\n\n\n\n13) The significance level Œ± represents:\n\n\n The p-value The probability H‚ÇÄ is true The probability of Type I error The power of the test\n\n\n\n\n14) For H‚ÇÄ: Œ≤‚ÇÇ = 0, if the 95% CI is [2.5, 7.5], then:\n\n\n p-value &gt; 0.05 p-value &lt; 0.05 We cannot reject H‚ÇÄ The estimate is unbiased\n\n\n\n\nHint\n\n0 is not in the interval, so we reject.\n\n\n\n15) In R, to get the p-value for a right-tailed test with t = 2.5 and df = 30:\n\n\n pt(2.5, 30) 2*pt(2.5, 30) 1 - pt(2.5, 30) pt(2.5, 30, lower.tail=TRUE)\n\n\n\n\n16) A larger sample size generally:\n\n\n Increases standard errors Decreases standard errors Doesn't affect hypothesis tests Increases the critical value\n\n\n\n\nHint\n\nMore data = more precision.\n\n\n\n17) The degrees of freedom for a simple regression (N=40) is:\n\n\n 40 39 38 37\n\n\n\n\nHint\n\ndf = N - K where K=2 (intercept + slope).\n\n\n\n18) If |t| &lt; t_critical, we:\n\n\n Reject H‚ÇÄ Fail to reject H‚ÇÄ Accept H‚ÇÅ Recalculate the test\n\n\n\n\n19) A one-tailed test is appropriate when:\n\n\n We don't know the direction We want more power Theory predicts a specific direction The sample size is small\n\n\n\n\n20) The t-distribution approaches the normal distribution as:\n\n\n Œ± decreases Œ± increases n decreases n increases\n\n\n\n\nHint\n\nWith large df, t ‚âà z.\n\n\n\n\n\n\nSummary\n\nHypothesis testing provides a formal framework for evaluating claims about parameters\nTest statistics measure how far estimates are from hypothesized values in standard error units\np-values quantify the strength of evidence against H‚ÇÄ\nConfidence intervals and hypothesis tests are mathematically equivalent\nLinear combinations require accounting for covariances in variance calculations\n\n\nüèÅ End of lab 9 üõë Remember to save your script üíæ\n\n\n\n\nReferences\nAcemoglu, D., Laibson, D., & List, J. A. (2015). Microeconomics (2nd ed.). Pearson. pp.¬†512‚Äì530. (For conceptual understanding of regression interpretation and causal inference basics.)\nColonescu, C. (2022). Principles of Econometrics with R (Version 2.0). Open Textbook Library. Chapters 7‚Äì9. (Used directly in labs, matches the PoEdata package and R examples.)\nGujarati, D. N., Porter, D. C., & Gunasekar, S. (2017). Basic Econometrics (5th ed.). McGraw-Hill Education. pp.¬†126‚Äì150, 175‚Äì195. (For theory and assumptions behind simple and multiple linear regression.)\nStock, J. H., & Watson, M. W. (2020). Introduction to Econometrics (4th ed.). Pearson. pp.¬†97‚Äì121, 145‚Äì165. (For hypothesis testing, p-values, and model diagnostics.)\n\n\n\nFormative Test (This test does NOT carry any marks)\n or link: MCQs test"
  }
]